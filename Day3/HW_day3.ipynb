{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee4f51c-e4db-4665-a559-c701fdbf47b5",
   "metadata": {},
   "source": [
    "## Intermediate Data Science\n",
    "\n",
    "#### University of Redlands - DATA 201\n",
    "#### Prof: Joanna Bieri [joanna_bieri@redlands.edu](mailto:joanna_bieri@redlands.edu)\n",
    "#### [Class Website: data201.joannabieri.com](https://joannabieri.com/data201_intermediate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9fdce6-e271-45a8-b39d-2cbb635d4a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Jupyter detected...\n",
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some basic package imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.defaule = 'colab'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56748c46-4a31-4738-b017-f60dfb91579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "^C\n",
      "Jupyter detected...\n",
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Jupyter detected...\n",
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\jesus\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - openpyxl\n",
      "    - xlrd\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openpyxl-3.1.5             |  py313hc624790_2         473 KB  conda-forge\n",
      "    xlrd-2.0.2                 |     pyhd8ed1ab_0          91 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         565 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  xlrd               conda-forge/noarch::xlrd-2.0.2-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openpyxl           pkgs/main::openpyxl-3.1.5-py313h827c3~ --> conda-forge::openpyxl-3.1.5-py313hc624790_2 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "openpyxl-3.1.5       | 473 KB    |            |   0% \n",
      "\n",
      "xlrd-2.0.2           | 91 KB     |            |   0% \u001b[A\n",
      "openpyxl-3.1.5       | 473 KB    | 3          |   3% \n",
      "openpyxl-3.1.5       | 473 KB    | ########## | 100% \n",
      "\n",
      "xlrd-2.0.2           | 91 KB     | #7         |  17% \u001b[A\n",
      "\n",
      "xlrd-2.0.2           | 91 KB     | ########## | 100% \u001b[A\n",
      "\n",
      "xlrd-2.0.2           | 91 KB     | ########## | 100% \u001b[A\n",
      "openpyxl-3.1.5       | 473 KB    | ########## | 100% \n",
      "openpyxl-3.1.5       | 473 KB    | ########## | 100% \n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Jupyter detected...\n",
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\jesus\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - requests\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    requests-2.32.5            |     pyhd8ed1ab_0          58 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          58 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  requests           pkgs/main/win-64::requests-2.32.3-py3~ --> conda-forge/noarch::requests-2.32.5-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "requests-2.32.5      | 58 KB     |            |   0% \n",
      "requests-2.32.5      | 58 KB     | ##7        |  28% \n",
      "requests-2.32.5      | 58 KB     | ########## | 100% \n",
      "requests-2.32.5      | 58 KB     | ########## | 100% \n",
      "                                                     \n",
      " done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Make sure that you load the new packages from lecture if needed\n",
    "!conda install -y lxml beautifulsoup4 html5lib\n",
    "!conda install -y openpyxl xlrd\n",
    "!conda install -y requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a98c1-f6c6-4915-9faf-7c878e0cd4fc",
   "metadata": {},
   "source": [
    "### You Try - 3 Warm-Up Problems From Lecture\n",
    "\n",
    "Here is a file that does not just read in nicely. See if you can use optional arguments to read it in.\n",
    "\n",
    "*Hint* How many (and which) rows of this data are just junk?\n",
    "\n",
    "**Terminal Command Line:**\n",
    "\n",
    "The command\n",
    "\n",
    "        cat data/ex4.csv\n",
    "\n",
    "if typed into a terminal prints out the contents of the file line by line. This lets us take a quick look at what is in the file. BEWARE - if you do this with a large file it will take a long time to print! Another great command is:\n",
    "\n",
    "        head data/ex4.csv\n",
    "\n",
    "would just show the first 10 lines of the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542020de-daf6-41fe-9b1e-d04966bbbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code lets you look at the data\n",
    "# the terminal command \"cat\" - prints the contents of a file\n",
    "# when we do !cat filename we can look at the \n",
    "file_name = 'data/ex4.csv'\n",
    "!cat data/ex4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bebf5-f4c6-4a5d-a85b-1e6cc466e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c524844-d646-4ecf-9b56-550fca9392b5",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3c1e2-ab82-47c1-8146-8ce6c53ae8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES - DICTIONARY TO PANDAS\n",
    "my_dict = {\"name\": \"Wes\",\n",
    " \"cities_lived\": [\"Akron\", \"Nashville\", \"New York\", \"San Francisco\"],\n",
    " \"pet\": None,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 34, \"hobbies\": [\"guitars\", \"soccer\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 42, \"hobbies\": [\"diving\", \"art\"]}]\n",
    "}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310943d-2085-40cb-a97a-5f481b8649f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in my_dict.keys():\n",
    "    print(key)\n",
    "    print(my_dict[key])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36aaab-6724-46ec-9fe2-6ee3abe1bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will give you an error\n",
    "df = pd.DataFrame(my_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98e4a3-5243-4fbe-891c-8083803a4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will work\n",
    "df = pd.DataFrame(my_dict['siblings'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108314b9-08cd-4ad4-a400-f16612f1bf5d",
   "metadata": {},
   "source": [
    "### You Try:\n",
    "\n",
    "Can you explain what is going on in the examples above? Why does one give an error and the other works? What specifically is it about focusing in on the siblings data that allows pandas to read this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c04cb-62c3-4fa7-8335-33008bdab4fd",
   "metadata": {},
   "source": [
    "**Your explanation here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57e65d-d87d-48eb-af95-e21558ba8412",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901faf7f-d773-4942-8c4c-9b7861bae16e",
   "metadata": {},
   "source": [
    "### You Try\n",
    "\n",
    "Here is an example website that contains a table:\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/\n",
    "\n",
    "1. Open the website in your browser. Does the page that appears contain ALL the data about hockey teams?\n",
    "2. How does the web address change when you select the second page of the website.\n",
    "3. See if you can write code that will scrape all of the data. HINT: I would use a for loop that updates the web address and appends the new table to a list.\n",
    "4. Once you have the list of tables can you get them into a single data frame and save the data as a .csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa12cab-9c94-4a07-8687-201a44eaf271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                Team Name  Year  Wins  Losses  OT Losses  Win %  \\\n",
       " 0           Boston Bruins  1990    44      24        NaN  0.550   \n",
       " 1          Buffalo Sabres  1990    31      30        NaN  0.388   \n",
       " 2          Calgary Flames  1990    46      26        NaN  0.575   \n",
       " 3      Chicago Blackhawks  1990    49      23        NaN  0.613   \n",
       " 4       Detroit Red Wings  1990    34      38        NaN  0.425   \n",
       " 5         Edmonton Oilers  1990    37      37        NaN  0.463   \n",
       " 6        Hartford Whalers  1990    31      38        NaN  0.388   \n",
       " 7       Los Angeles Kings  1990    46      24        NaN  0.575   \n",
       " 8   Minnesota North Stars  1990    27      39        NaN  0.338   \n",
       " 9      Montreal Canadiens  1990    39      30        NaN  0.487   \n",
       " 10      New Jersey Devils  1990    32      33        NaN  0.400   \n",
       " 11     New York Islanders  1990    25      45        NaN  0.312   \n",
       " 12       New York Rangers  1990    36      31        NaN  0.450   \n",
       " 13    Philadelphia Flyers  1990    33      37        NaN  0.412   \n",
       " 14    Pittsburgh Penguins  1990    41      33        NaN  0.512   \n",
       " 15       Quebec Nordiques  1990    16      50        NaN  0.200   \n",
       " 16        St. Louis Blues  1990    47      22        NaN  0.588   \n",
       " 17    Toronto Maple Leafs  1990    23      46        NaN  0.287   \n",
       " 18      Vancouver Canucks  1990    28      43        NaN  0.350   \n",
       " 19    Washington Capitals  1990    37      36        NaN  0.463   \n",
       " 20          Winnipeg Jets  1990    26      43        NaN  0.325   \n",
       " 21          Boston Bruins  1991    36      32        NaN  0.450   \n",
       " 22         Buffalo Sabres  1991    31      37        NaN  0.388   \n",
       " 23         Calgary Flames  1991    31      37        NaN  0.388   \n",
       " 24     Chicago Blackhawks  1991    36      29        NaN  0.450   \n",
       " \n",
       "     Goals For (GF)  Goals Against (GA)  + / -  \n",
       " 0              299                 264     35  \n",
       " 1              292                 278     14  \n",
       " 2              344                 263     81  \n",
       " 3              284                 211     73  \n",
       " 4              273                 298    -25  \n",
       " 5              272                 272      0  \n",
       " 6              238                 276    -38  \n",
       " 7              340                 254     86  \n",
       " 8              256                 266    -10  \n",
       " 9              273                 249     24  \n",
       " 10             272                 264      8  \n",
       " 11             223                 290    -67  \n",
       " 12             297                 265     32  \n",
       " 13             252                 267    -15  \n",
       " 14             342                 305     37  \n",
       " 15             236                 354   -118  \n",
       " 16             310                 250     60  \n",
       " 17             241                 318    -77  \n",
       " 18             243                 315    -72  \n",
       " 19             258                 258      0  \n",
       " 20             260                 288    -28  \n",
       " 21             270                 275     -5  \n",
       " 22             289                 299    -10  \n",
       " 23             296                 305     -9  \n",
       " 24             257                 236     21  ]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is how I could get one page\n",
    "website = 'https://www.scrapethissite.com/pages/forms/'\n",
    "tables = pd.read_html(website)\n",
    "len(tables)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5a8fd-8b99-46a6-902c-aded7c2144e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#1. Open the website in your browser. Does the page that appears contain ALL the data about hockey teams?\n",
    "\n",
    "#When I open the website in my web browser I see a table and pages to look at. So no, the page that appears does NOT contain\n",
    "#all the data about hocket teams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88855f2-fe1c-4adb-9a63-c3c420e17eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. How does the web address change when you select the second page of the website.\n",
    "\n",
    "#When you go from page 1 to page 2, 3, 4, 5, .... 23 the only thing that changes is the page\n",
    "#number in the website. For example, this is the link https://www.scrapethissite.com/pages/forms/?page_num=1, and \n",
    "#what changes when I move from page 1.2.3 is /?page_num=1,2,3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e38fc70-0c3c-4133-b18c-533ef373c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. See if you can write code that will scrape all of the data. HINT: I would use a for loop that updates\n",
    "#the web address and appends the new table to a list.\n",
    "df_list = []\n",
    "#There are 24 pages in the website and we must loop through all \n",
    "website = 'https://www.scrapethissite.com/pages/forms/'\n",
    "website24 ='https://www.scrapethissite.com/pages/forms/?page_num=23'\n",
    "\n",
    "for pg in range(1,24):\n",
    "    url = f\"{website}?page_num={pg}\"\n",
    "    tables = pd.read_html(url)\n",
    "    hockey_page = tables[0]\n",
    "    df_list.append(hockey_page)\n",
    "\n",
    "\n",
    "#This code scraped 24 pages on the website by slightly adjusting the url. The data is then put on\n",
    "# a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a62c16-23e6-4d49-be69-0f238095728f",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aafc3c04-e4ea-4a26-a3e0-c974b81752cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Once you have the list of tables can you get them into a single data frame and save the data as a .csv?\n",
    "#I use concat to stack the different tables to get this functional. help(pd) emphasized the importance of\n",
    "#ignore_index, especially since I did not want to create a new index. \n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "final_df.to_csv('Hockey_Teams.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43717785-d714-407f-a737-4ca75822a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4c4e3-f4e8-490c-9490-f8ed3e55d039",
   "metadata": {},
   "source": [
    "## Reading and Writing Data - Day3 HW\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "Go to Kaggle Datasets: https://www.kaggle.com/datasets\n",
    "\n",
    "Find a data set that you are interested in looking at. You are welcome to work together and choose a data set as a group! You should read in this data and do some basic statistics on the data set. Answer the following questions:\n",
    "\n",
    "1. Tell your reader about the data: Where did you get it? When did you access it? Who owns it? What is the license? Are there any acknowledgments that you should give for using the data? All of this should be on the Kaggle page\n",
    "2. How many variables and observations?\n",
    "3. What type of data is contained? Was it read in as strings, ints, floats?\n",
    "4. Are there any NaNs or weird data types that you can see?\n",
    "5. Most Kaggle datasets contain some basic stats or visualizations on the download page. See if you can recreate some of the plots or data you see on the website.\n",
    "6. Come up with at least one question of your own that you can answer by analyzing the data.\n",
    "7. Create a dataframe with just the data you need to answer your question - save the data subset to a file (your choice of type)\n",
    "8. **In a NEW NOTEBOOK** Write code that reads in your subset of the data, markdown that explains clearly where you got the data originally (license and references included) and the process you took to create your subset, a description of the question you are answering, and code that can reliably run and answer your question followed by words that explain your results.\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "Your final notebooks should:\n",
    "\n",
    "- [ ] Be completely new notebooks with just the Day3 stuff in it: First the code that creates your data and second the code that reads in the data and does the analysis. \n",
    "- [ ] Be reproducible with junk code removed.\n",
    "- [ ] Have lots of language describing what you are doing, especially for questions you are asking or things that you find interesting about the data. Use complete sentences, nice headings, and good markdown formatting: https://www.markdownguide.org/cheat-sheet/\n",
    "- [ ] It should run without errors from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567572e-fc8c-4558-98b1-7363924921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DONT FORGET TO HAND IN YOUR DATA!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
